{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d31b2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Day 5: Evaluation\n",
    "Today we are going to explore the ways we can evaluate our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03b8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's add the markdown download code here\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md')\n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2fed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from course!\n",
      "FAQ documents: 3\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello from course!\")\n",
    "balsam_faq = read_repo_data('Zesky665', 'balsam')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "    \n",
    "print(f\"FAQ documents: {len(balsam_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e888fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can start chunking\n",
    "# There are multiple ways of chunking a document.\n",
    "# Here they are in order of complexity:\n",
    "# - Simple Chunking\n",
    "# - Token Based Chunking\n",
    "# - Sematinc Chunking\n",
    "# - Paragrapgh Splitting\n",
    "# - Section Splitting\n",
    "# - AI-powered Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d59cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most commonly used simple chunking method is sliding window, which is chunking with overlap.\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be4304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy['content']\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd29cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section Splitting\n",
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc25c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "\n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "\n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8232e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e8567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Based Chunking\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# api_key=os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def llm(prompt, model=\"mistral-large-latest\"):\n",
    "    chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the best French cheese?\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    print(chat_response.choices[0].message.content)\n",
    "    return chat_response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4208e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3da327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt_template)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f28d04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defab334c6c344e29caa3e7ed28905b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m doc_copy = doc.copy()\n\u001b[32m      7\u001b[39m doc_content = doc_copy.pop(\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m sections = \u001b[43mintelligent_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m section \u001b[38;5;129;01min\u001b[39;00m sections:\n\u001b[32m     11\u001b[39m     section_doc = doc_copy.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mintelligent_chunking\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintelligent_chunking\u001b[39m(text):\n\u001b[32m      2\u001b[39m     prompt = prompt_template.format(document=text)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     sections = response.split(\u001b[33m'\u001b[39m\u001b[33m---\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     sections = [s.strip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sections \u001b[38;5;28;01mif\u001b[39;00m s.strip()]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mllm\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mmistral-large-latest\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     chat_response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the best French cheese?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(chat_response.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/mistralai/chat.py:222\u001b[39m, in \u001b[36mChat.complete\u001b[39m\u001b[34m(self, model, messages, temperature, top_p, max_tokens, stream, stop, random_seed, response_format, tools, tool_choice, presence_penalty, frequency_penalty, n, prediction, parallel_tool_calls, prompt_mode, safe_prompt, retries, server_url, timeout_ms, http_headers)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(retries, utils.RetryConfig):\n\u001b[32m    220\u001b[39m     retry_config = (retries, [\u001b[33m\"\u001b[39m\u001b[33m429\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m500\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m502\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m503\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m504\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m http_res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhook_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHookContext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msdk_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat_completion_v1_chat_completions_post\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43moauth2_scopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43msecurity_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_security_from_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msdk_configuration\u001b[49m\u001b[43m.\u001b[49m\u001b[43msecurity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSecurity\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m422\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4XX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m5XX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m response_data: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m utils.match_response(http_res, \u001b[33m\"\u001b[39m\u001b[33m200\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/mistralai/basesdk.py:275\u001b[39m, in \u001b[36mBaseSDK.do_request\u001b[39m\u001b[34m(self, hook_ctx, request, error_status_codes, stream, retry_config)\u001b[39m\n\u001b[32m    273\u001b[39m     http_res = utils.retry(do, utils.Retries(retry_config[\u001b[32m0\u001b[39m], retry_config[\u001b[32m1\u001b[39m]))\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     http_res = \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m utils.match_status_codes(error_status_codes, http_res.status_code):\n\u001b[32m    278\u001b[39m     http_res = hooks.after_success(AfterSuccessContext(hook_ctx), http_res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/mistralai/basesdk.py:238\u001b[39m, in \u001b[36mBaseSDK.do_request.<locals>.do\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    236\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mclient is required\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     http_res = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    240\u001b[39m     _, e = hooks.after_error(AfterErrorContext(hook_ctx), \u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ai_agent_crash_course/aihero/Day_2/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1234\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1231\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1232\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1233\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1107\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search\n",
    "# Text, Vector and Sematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e65662c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7943eaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1115f3b60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Search\n",
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7afd10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d1e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acc16eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d61aad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d7f1df092343819fcd2832b6fa452c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf28f2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x323864b00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5561eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8de9668c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fea65b01e22499fa57762b94e3f6fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x323858cb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0017d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0156ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "\n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2ae44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents and Tool Use\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# api_key= os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-small-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def llm(prompt, model=\"mistral-small-latest\"):\n",
    "    chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    print(chat_response.choices[0].message.content)\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f838ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes! You can join the course now. Most online courses allow you to enroll at any time, and you can start learning immediately. However, here are a few things to check:\n",
      "\n",
      "1. **Course Start Date** – Some courses have fixed start dates (especially if they're instructor-led or cohort-based). If that's the case, you may need to wait for the next intake.\n",
      "2. **Self-Paced vs. Scheduled** – If the course is self-paced, you can begin right away. If it's scheduled, check the next available session.\n",
      "3. **Enrollment Deadline** – Some courses have a cutoff date for joining.\n",
      "\n",
      "If you're unsure, check the course description or contact the course provider for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes! You can join the course now. Most online courses allow you to enroll at any time, and you can start learning immediately. However, here are a few things to check:\\n\\n1. **Course Start Date** – Some courses have fixed start dates (especially if they're instructor-led or cohort-based). If that's the case, you may need to wait for the next intake.\\n2. **Self-Paced vs. Scheduled** – If the course is self-paced, you can begin right away. If it's scheduled, check the next available session.\\n3. **Enrollment Deadline** – Some courses have a cutoff date for joining.\\n\\nIf you're unsure, check the course description or contact the course provider for details.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "resp = llm(user_prompt, model)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d83872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfe665e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"text_search\",\n",
    "        \"description\": \"Search the FAQ database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7c5cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "tools = [text_search_tool]\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = chat_messages,\n",
    "        tools = tools,\n",
    "        tool_choice = \"any\",\n",
    "        parallel_tool_calls = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce1d7aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='83e53a81a7504ca3a985bbc30a2f99bf', object='chat.completion', model='mistral-small-latest', usage=UsageInfo(prompt_tokens=104, completion_tokens=16, total_tokens=120, prompt_audio_seconds=Unset()), created=1759521877, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='text_search', arguments='{\"query\": \"can I join the course now\"}'), id='fUxnup9U2', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd059b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "call = chat_response.choices[0].message.tool_calls[0]\n",
    "\n",
    "arguments = json.loads(call.function.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.id,\n",
    "    \"output\": json.dumps(result),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9b0bdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function_call_output',\n",
       " 'call_id': 'fUxnup9U2',\n",
       " 'output': '[{\"id\": \"3f1424af17\", \"question\": \"Course: Can I still join the course after the start date?\", \"sort_order\": 3, \"content\": \"Yes, even if you don\\'t register, you\\'re still eligible to submit the homework.\\\\n\\\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don\\'t leave everything for the last minute.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\"}, {\"id\": \"9e508f2212\", \"question\": \"Course: When does the course start?\", \"sort_order\": 1, \"content\": \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\\\n\\\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\\\n- Don\\\\u2019t forget to register in DataTalks.Club\\'s Slack and join the channel.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\"}, {\"id\": \"068529125b\", \"question\": \"Course - Can I follow the course after it finishes?\", \"sort_order\": 8, \"content\": \"Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\\\n\\\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\"}, {\"id\": \"33fc260cd8\", \"question\": \"Course: What can I do before the course starts?\", \"sort_order\": 5, \"content\": \"Start by installing and setting up all the dependencies and requirements:\\\\n\\\\n- Google Cloud account\\\\n- Google Cloud SDK\\\\n- Python 3 (installed with Anaconda)\\\\n- Terraform\\\\n- Git\\\\n\\\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md\"}, {\"id\": \"c207b8614e\", \"question\": \"Course: Can I get support if I take the course in the self-paced mode?\", \"sort_order\": 9, \"content\": \"Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\\\n\\\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don\\\\u2019t rely on its answers 100%.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\"}]'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cfa8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='2af5cf5bd98f4a939682f365ba60b563' object='chat.completion' model='mistral-small-latest' usage=UsageInfo(prompt_tokens=946, completion_tokens=50, total_tokens=996, prompt_audio_seconds=Unset()) created=1759521881 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n"
     ]
    }
   ],
   "source": [
    "# Append the assistant's message with tool calls\n",
    "chat_messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": [call]\n",
    "})\n",
    "\n",
    "# Append the tool result\n",
    "chat_messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"name\": call.function.name,\n",
    "    \"content\": call_output[\"output\"],\n",
    "    \"tool_call_id\": call.id\n",
    "})\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = chat_messages,\n",
    "        tools = tools,\n",
    "        tool_choice = \"auto\",\n",
    "        parallel_tool_calls = False,\n",
    "    )\n",
    "\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d51a26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = chat_messages,\n",
    "        tools = tools,\n",
    "        tool_choice = \"auto\",\n",
    "        parallel_tool_calls = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb4239d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='7014c320879f408cafd86b3f2ed0ff5d', object='chat.completion', model='mistral-small-latest', usage=UsageInfo(prompt_tokens=104, completion_tokens=16, total_tokens=120, prompt_audio_seconds=Unset()), created=1759521884, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='text_search', arguments='{\"query\": \"can I join the course now\"}'), id='fX8tZUjTf', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f16244f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58384906",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Always search for relevant information before answering.\n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da0cbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccc2c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "import os\n",
    "os.environ['MISTRAL_API_KEY'] = api_key\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='mistral:mistral-small-latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5dc1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2dca8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"Yes, you can still join the course after the start date. You can submit homework even if you don't register, but be aware of the deadlines for homework and final projects. Don't leave everything for the last minute.\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eab74b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2025, 10, 3, 20, 5, 6, 369912, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course.\\n\\nAlways search for relevant information before answering.\\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\": \"can I join the course now\"}', tool_call_id='X6DOnnioH')], usage=RequestUsage(input_tokens=179, output_tokens=16), model_name='mistral-small-latest', timestamp=datetime.datetime(2025, 10, 3, 20, 5, 6, tzinfo=TzInfo(UTC)), provider_name='mistral', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='5dbb23aafd1b433bbcfe14f15bd22435', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}], tool_call_id='X6DOnnioH', timestamp=datetime.datetime(2025, 10, 3, 20, 5, 6, 723410, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course.\\n\\nAlways search for relevant information before answering.\\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[TextPart(content=\"Yes, you can still join the course after the start date. You can submit homework even if you don't register, but be aware of the deadlines for homework and final projects. Don't leave everything for the last minute.\")], usage=RequestUsage(input_tokens=971, output_tokens=47), model_name='mistral-small-latest', timestamp=datetime.datetime(2025, 10, 3, 20, 5, 6, tzinfo=TzInfo(UTC)), provider_name='mistral', provider_details={'finish_reason': 'stop'}, provider_response_id='ba5acfbd73724bfd8e56bffb5863ee62', finish_reason='stop')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ede40492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4ff1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course.\n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='mistral:mistral-small-latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "17318f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f5e30442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fab55695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d357d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I couldn't find specific information on how to run Flink in Python from the course materials. However, I can provide you with general guidance on the topic.\n",
      "\n",
      "Apache Flink is a powerful framework for distributed stream processing, and while it is primarily written in Java and Scala, it also has support for Python through PyFlink. Here are the general steps to run Flink in Python:\n",
      "\n",
      "1. **Install PyFlink**: You can install PyFlink using pip. Open your terminal or command prompt and run:\n",
      "   ```bash\n",
      "   pip install apache-flink\n",
      "   ```\n",
      "\n",
      "2. **Set Up Your Environment**: Make sure you have Java installed, as Flink requires it. You can download and install Java from the [Oracle website](https://www.oracle.com/java/technologies/javase-downloads.html) or use an open-source version like OpenJDK.\n",
      "\n",
      "3. **Write Your Flink Program**: Create a Python script for your Flink program. Here is a simple example:\n",
      "   ```python\n",
      "   from pyflink.datastream import StreamExecutionEnvironment\n",
      "   from pyflink.datastream.functions import MapFunction\n",
      "   from pyflink.datastream.connectors import StreamingFileSink\n",
      "   from pyflink.datastream.connectors.file_system import OutputFileConfig, RollingPolicy\n",
      "\n",
      "   # Create a StreamExecutionEnvironment\n",
      "   env = StreamExecutionEnvironment.get_execution_environment()\n",
      "\n",
      "   # Define a simple data source\n",
      "   text = env.from_collection([\"Hello\", \"Flink\", \"Python\"])\n",
      "\n",
      "   # Apply a transformation\n",
      "   result = text.map(lambda value: (value, 1))\n",
      "\n",
      "   # Define a sink to print the results\n",
      "   result.print()\n",
      "\n",
      "   # Execute the program\n",
      "   env.execute(\"Python Flink Example\")\n",
      "   ```\n",
      "\n",
      "4. **Run Your Flink Program**: You can run your Python script directly from the command line:\n",
      "   ```bash\n",
      "   python your_flink_script.py\n",
      "   ```\n",
      "\n",
      "5. **Advanced Configuration**: For more advanced configurations, such as running Flink on a cluster, you may need to set up a Flink cluster and submit your job to it. This involves more detailed setup and configuration, which you can find in the [official Flink documentation](https://nightlies.apache.org/flink/flink-docs-stable/docs/try-flink/local_installation/).\n",
      "\n",
      "For more detailed information and advanced use cases, you can refer to the [PyFlink documentation](https://nightlies.apache.org/flink/flink-docs-stable/docs/try-flink/pyflink/api/).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_20251003_202910_e92585.json')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6d4ea757",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.\n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='mistral:mistral-small-latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d208373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met.\n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do\n",
    "- answer_relevant: The response directly addresses the user's question\n",
    "- answer_clear: The answer is clear and correct\n",
    "- answer_citations: The response includes proper citations or sources when required\n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked?\n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "67e32f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v3\",\n",
    "    instructions=evaluation_prompt,\n",
    "    tools=[text_search],\n",
    "    model='mistral:mistral-small-latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "614e40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c54f15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='mistral:mistral-small-latest',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "edd8e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a53a0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "630cd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_record = load_log_file('/Users/zhare/Documents/GitHub/ai_agent_crash_course/aihero/Day_5/logs/faq_agent_20251003_202910_e92585.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6cc4aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent's response is highly evaluated. It followed instructions, provided a relevant, clear, and complete answer, included proper citations, and used the search tool effectively.\n",
      "check_name='instructions_follow' justification='The agent used the search tool to find relevant information from the course materials before answering the question.' check_pass=True\n",
      "check_name='instructions_avoid' justification='The agent did not do anything it was told not to do.' check_pass=True\n",
      "check_name='answer_relevant' justification=\"The response directly addresses the user's question about running Flink in Python.\" check_pass=True\n",
      "check_name='answer_clear' justification='The answer is clear and provides a step-by-step guide on how to run Flink in Python.' check_pass=True\n",
      "check_name='answer_citations' justification='The response includes links to official documentation for further reference.' check_pass=True\n",
      "check_name='completeness' justification='The response covers all key aspects of the request, including installation, setup, writing a Flink program, running the program, and advanced configuration.' check_pass=True\n",
      "check_name='tool_call_search' justification='The search tool was invoked to find relevant information from the course materials.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2f8eba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "\n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "\n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "\n",
    "            parts.append(part)\n",
    "\n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "\n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a4060a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output\n",
    "\n",
    "\n",
    "log_record = load_log_file('/Users/zhare/Documents/GitHub/ai_agent_crash_course/aihero/Day_5/logs/faq_agent_20251003_202910_e92585.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aaf8c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='mistral:mistral-small-latest',\n",
    "    output_type=QuestionsList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4e9ca5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8bac831d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3958e3d1f8b64924884ff3e7b0e72fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I create a new branch to edit in dbt? Are there any resources available to guide me through this process?\n",
      "To create a new branch in dbt (data build tool), you'll typically want to use Git, the version control system, rather than dbt itself. Here's a step-by-step guide:\n",
      "\n",
      "1. **Open your terminal or command prompt**: Navigate to your dbt project directory.\n",
      "\n",
      "2. **Check your current branch**: Before creating a new branch, it's good practice to check which branch you're currently on. You can do this by using the command `git branch`. The current branch will be highlighted with an asterisk (*).\n",
      "\n",
      "3. **Create a new branch**: To create a new branch, use the command `git checkout -b <branch-name>`. Replace `<branch-name>` with the name you want to give to your new branch. This command will create a new branch and switch you to it.\n",
      "\n",
      "4. **Push the new branch to the remote repository**: If you want to push your new branch to the remote repository (e.g., GitHub, GitLab, Bitbucket), use the command `git push -u origin <branch-name>`. This will push your new branch to the remote repository and set the upstream tracking.\n",
      "\n",
      "Remember to replace `<branch-name>` with your desired branch name in the commands above.\n",
      "\n",
      "As for resources, here are a few that might help:\n",
      "\n",
      "1. **dbt Documentation**: The official dbt documentation is a great place to start. It provides detailed information on how to use dbt and its features.\n",
      "\n",
      "2. **Git Documentation**: The official Git documentation is a comprehensive resource for learning about Git and its commands.\n",
      "\n",
      "3. **Interactive Git Tutorials**: Websites like [Learn Git Branching](https://learngitbranching.js.org/) offer interactive tutorials that can help you understand and practice Git commands, including creating and managing branches.\n",
      "\n",
      "4. **dbt Community Slack**: The dbt Community Slack is a place where you can ask questions and learn from other dbt users. You can find the invitation link on the dbt website.\n",
      "\n",
      "5. **Online Courses and Tutorials**: Websites like Udemy, Coursera, and LinkedIn Learning offer courses on dbt and Git that can help you learn these tools in a structured way.\n",
      "\n",
      "The audio quality in the course videos is hard to hear. Do you have any suggestions for improving it?\n",
      "I'm sorry to hear that you're having trouble with the audio quality in the course videos. Unfortunately, I don't have the tools to directly address this issue. However, I can suggest a few general tips that might help improve your listening experience:\n",
      "\n",
      "1. **Check Your Internet Connection**: Ensure that your internet connection is stable, as this can affect the streaming quality of the videos.\n",
      "2. **Use Headphones**: Sometimes, using headphones can provide a clearer audio experience compared to speakers.\n",
      "3. **Adjust Video Quality**: If the platform allows, try lowering the video quality to see if this improves the audio.\n",
      "4. **Contact Support**: Reach out to the course provider's support team for assistance. They may have specific solutions or alternatives for you.\n",
      "\n",
      "I'm having trouble building the `fact_trips.sql` model. The issue seems to be with the `payment_type_description` field. How can I resolve this?\n",
      "I'm sorry to hear that you're having trouble with building the `fact_trips.sql` model, specifically with the `payment_type_description` field. Unfortunately, I don't have access to the necessary tools to provide a specific solution for your issue. However, I can suggest some general steps you might take to troubleshoot the problem:\n",
      "\n",
      "1. **Check for Typos**: Ensure that there are no typos in your SQL code, especially around the `payment_type_description` field.\n",
      "2. **Verify Data Types**: Make sure that the data type of `payment_type_description` is compatible with the operations you're performing.\n",
      "3. **Review Dependencies**: Check if there are any dependencies or constraints associated with the `payment_type_description` field that might be causing the issue.\n",
      "4. **Consult Documentation**: Look up the documentation or any available resources related to the software or database you're using.\n",
      "5. **Seek Community Help**: Consider reaching out to community forums or support groups where you might find others who have encountered similar issues.\n",
      "\n",
      "If you provide more details about the error message or the specific problem you're encountering, I might be able to offer more targeted advice.\n",
      "\n",
      "I'm encountering a port conflict when trying to run a service on port 8080. How can I free up this port?\n",
      "I'm sorry to hear you're experiencing a port conflict. Here are a few steps you can take to free up port 8080:\n",
      "\n",
      "1. **Identify the Process Using the Port**: You can use the `netstat` command to identify which process is using the port. On Windows, you can use `netstat -ano | findstr :8080` and on Unix-based systems, you can use `netstat -tuln | grep 8080`.\n",
      "\n",
      "2. **Terminate the Process**: Once you've identified the process, you can terminate it. On Windows, you can use the `Taskkill` command with the process ID (PID) that you found in the previous step. On Unix-based systems, you can use the `kill` command with the PID.\n",
      "\n",
      "3. **Change the Port**: If you can't terminate the process or if it's a system process, you might want to consider changing the port that your service is trying to use. You can usually do this in the configuration files of your service.\n",
      "\n",
      "4. **Use a Different Port**: If changing the port isn't an option, you might want to consider using a different port for your service. You can choose a port that's not currently in use.\n",
      "\n",
      "How can I check if the BigQuery Command Line Tool is installed on my system?\n",
      "To check if the BigQuery Command Line Tool is installed on your system, you can follow these steps:\n",
      "\n",
      "1. **Open a Terminal or Command Prompt:**\n",
      "   - On Windows, you can open the Command Prompt by pressing `Win + R`, typing `cmd`, and pressing Enter.\n",
      "   - On macOS or Linux, you can open the Terminal application.\n",
      "\n",
      "2. **Run the BigQuery Command:**\n",
      "   - Type the following command and press Enter:\n",
      "     ```sh\n",
      "     bq\n",
      "     ```\n",
      "   - If the BigQuery Command Line Tool is installed, you should see output related to the BigQuery command-line interface, including usage instructions and options.\n",
      "   - If you see a message indicating that the command is not found (e.g., `'bq' is not recognized as an internal or external command` on Windows, or `command not found` on macOS/Linux), the tool is not installed.\n",
      "\n",
      "3. **Check the Installation Path (Optional):**\n",
      "   - If you suspect the tool might be installed but not in your system's PATH, you can try locating it manually. The installation directory might be something like:\n",
      "     - On Windows: `C:\\Program Files\\Google\\Cloud SDK\\google-cloud-sdk\\bin`\n",
      "     - On macOS/Linux: `/google-cloud-sdk/bin` in your home directory or another location where you installed the Google Cloud SDK.\n",
      "\n",
      "4. **Verify Installation via Google Cloud SDK:**\n",
      "   - The BigQuery Command Line Tool is often included with the Google Cloud SDK. You can check if the Google Cloud SDK is installed by running:\n",
      "     ```sh\n",
      "     gcloud\n",
      "     ```\n",
      "   - If the Google Cloud SDK is installed, you can install the BigQuery Command Line Tool by running:\n",
      "     ```sh\n",
      "     gcloud components install bq\n",
      "     ```\n",
      "\n",
      "By following these steps, you can determine whether the BigQuery Command Line Tool is installed on your system.\n",
      "\n",
      "What are the prerequisites for this data engineering course? Do I need prior experience in data engineering?\n",
      "I'm sorry, but I don't have access to the specific information needed to answer your question about the prerequisites for the data engineering course. However, I can suggest that you check the course website or contact the course provider directly for the most accurate and detailed information. They should be able to provide you with insights into whether prior experience in data engineering is required.\n",
      "\n",
      "How can I set the `Project subdirectory` in dbt cloud? I can't seem to find the option.\n",
      "To set the `Project subdirectory` in dbt Cloud, follow these steps:\n",
      "\n",
      "1. Navigate to the projects window on dbt Cloud.\n",
      "2. Go to Settings -> Edit.\n",
      "3. Add the directory path where the dbt project is located. Ensure that this path matches your file explorer path. For example:\n",
      "\n",
      "   ```\n",
      "   /week5/taxi_rides_ny\n",
      "   ```\n",
      "\n",
      "4. Check that there are no files waiting to be committed to GitHub if you’re running the job to deploy to PROD.\n",
      "\n",
      "For more detailed instructions, refer to the [DBT Deploy: This dbt Cloud run was cancelled because a valid dbt project was not found](faq-main/_questions/data-engineering-zoomcamp/module-4/067_dc51ef9830_dbt-deploy-this-dbt-cloud-run-was-cancelled-becaus.md) guide.\n",
      "\n",
      "---\n",
      "\n",
      "### Checklist Evaluation\n",
      "\n",
      "- **instructions_follow**: true - The response provides steps to set the `Project subdirectory` in dbt Cloud.\n",
      "- **instructions_avoid**: true - There are no instructions to avoid in this context.\n",
      "- **answer_relevant**: true - The response directly addresses the user's question about setting the `Project subdirectory`.\n",
      "- **answer_clear**: true - The answer is clear and provides a step-by-step guide.\n",
      "- **answer_citations**: true - The response includes a reference to a detailed guide.\n",
      "- **completeness**: true - The response covers all key aspects of setting the `Project subdirectory`.\n",
      "- **tool_call_search**: true - The search tool was invoked to find relevant information.\n",
      "\n",
      "I'm having trouble accessing Dataproc in my Google Cloud project. What steps can I take to resolve this issue?\n",
      "I'm sorry to hear that you're having trouble accessing Dataproc. Unfortunately, I don't have the necessary tools or information to provide you with specific steps to resolve this issue. However, I can suggest that you check the Google Cloud status page for any ongoing outages or issues, ensure that your project has the necessary permissions and APIs enabled, and consult the Google Cloud documentation or support for further assistance.\n",
      "\n",
      "I'm trying to create a new server in pgAdmin but the create server dialog is not appearing. What should I do?\n",
      "### pgAdmin: Create Server Dialog Does Not Appear\n",
      "\n",
      "pgAdmin has a new version. The create server dialog may not appear. Try using `Register` -> `Server` instead.\n",
      "\n",
      "### Additional Resources\n",
      "\n",
      "- [How to Persist pgAdmin Configurations](faq-main/_questions/data-engineering-zoomcamp/module-1/081_ca3b8ac8db_pgadmin-how-to-persist-pgadmin-configurations.md)\n",
      "- [Accessing pgAdmin Address via Browser](faq-main/_questions/data-engineering-zoomcamp/module-1/080_2efd03d7f8_pgadmin-can-not-accessopen-the-pgadmin-address-via.md)\n",
      "- [Running pgAdmin in Docker with GitHub Codespaces](faq-main/_questions/data-engineering-zoomcamp/module-1/009_3d24f7796d_github-codespaces-running-pgadmin-in-docker.md)\n",
      "- [Persisting PGAdmin Docker Contents on GCP](faq-main/_questions/data-engineering-zoomcamp/module-1/042_ac52bea382_docker-compose-persist-pgadmin-docker-contents-on.md)\n",
      "\n",
      "Can I set up the course environment on my local machine? Are there any specific challenges for Windows users? What are the alternatives if I prefer not to work locally?\n",
      "To evaluate the quality of the AI agent's answer, let's go through the checklist:\n",
      "\n",
      "1. **instructions_follow**: The agent followed the user's instructions to provide information on setting up the course environment locally, challenges for Windows users, and alternatives to working locally.\n",
      "   - **Judgment**: true\n",
      "   - **Explanation**: The agent addressed all parts of the user's question.\n",
      "\n",
      "2. **instructions_avoid**: The agent avoided doing things it was told not to do.\n",
      "   - **Judgment**: true\n",
      "   - **Explanation**: There were no specific instructions to avoid, but the agent did not provide any irrelevant information.\n",
      "\n",
      "3. **answer_relevant**: The response directly addresses the user's question about setting up the course environment locally, challenges for Windows users, and alternatives to working locally.\n",
      "   - **Judgment**: true\n",
      "   - **Explanation**: The agent provided relevant information on all aspects of the user's question.\n",
      "\n",
      "4. **answer_clear**: The answer is clear and correct.\n",
      "   - **Judgment**: true\n",
      "   - **Explanation**: The agent's response was clear and provided correct information.\n",
      "\n",
      "5. **answer_citations**: The response includes proper citations or sources when required.\n",
      "   - **Judgment**: false\n",
      "   - **Explanation**: The agent did not provide any citations or sources for the information given.\n",
      "\n",
      "6. **completeness**: The response is complete and covers all key aspects of the request.\n",
      "   - **Judgment**: true\n",
      "   - **Explanation**: The agent covered all parts of the user's question comprehensively.\n",
      "\n",
      "7. **tool_call_search**: Is the search tool invoked?\n",
      "   - **Judgment**: false\n",
      "   - **Explanation**: The agent did not invoke the search tool to gather more information.\n",
      "\n",
      "Overall, the AI agent's answer was comprehensive, relevant, and clear, but it lacked citations or sources for the information provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5b329790",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v3' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eed94de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': 'How can I check if the BigQuery Command Line Tool is installed on my system?',\n",
       "      'timestamp': '2025-10-03T20:30:28.738647+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"To check if the BigQuery Command Line Tool is installed on your system, you can follow these steps:\\n\\n1. **Open a Terminal or Command Prompt:**\\n   - On Windows, you can open the Command Prompt by pressing `Win + R`, typing `cmd`, and pressing Enter.\\n   - On macOS or Linux, you can open the Terminal application.\\n\\n2. **Run the BigQuery Command:**\\n   - Type the following command and press Enter:\\n     ```sh\\n     bq\\n     ```\\n   - If the BigQuery Command Line Tool is installed, you should see output related to the BigQuery command-line interface, including usage instructions and options.\\n   - If you see a message indicating that the command is not found (e.g., `'bq' is not recognized as an internal or external command` on Windows, or `command not found` on macOS/Linux), the tool is not installed.\\n\\n3. **Check the Installation Path (Optional):**\\n   - If you suspect the tool might be installed but not in your system's PATH, you can try locating it manually. The installation directory might be something like:\\n     - On Windows: `C:\\\\Program Files\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\bin`\\n     - On macOS/Linux: `/google-cloud-sdk/bin` in your home directory or another location where you installed the Google Cloud SDK.\\n\\n4. **Verify Installation via Google Cloud SDK:**\\n   - The BigQuery Command Line Tool is often included with the Google Cloud SDK. You can check if the Google Cloud SDK is installed by running:\\n     ```sh\\n     gcloud\\n     ```\\n   - If the Google Cloud SDK is installed, you can install the BigQuery Command Line Tool by running:\\n     ```sh\\n     gcloud components install bq\\n     ```\\n\\nBy following these steps, you can determine whether the BigQuery Command Line Tool is installed on your system.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 317,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 390,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:28+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': '791e0ef101954338aea930bea6c51ba8',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203028_196977.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': 'How do I create a new branch to edit in dbt? Are there any resources available to guide me through this process?',\n",
       "      'timestamp': '2025-10-03T20:30:17.009417+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"To create a new branch in dbt (data build tool), you'll typically want to use Git, the version control system, rather than dbt itself. Here's a step-by-step guide:\\n\\n1. **Open your terminal or command prompt**: Navigate to your dbt project directory.\\n\\n2. **Check your current branch**: Before creating a new branch, it's good practice to check which branch you're currently on. You can do this by using the command `git branch`. The current branch will be highlighted with an asterisk (*).\\n\\n3. **Create a new branch**: To create a new branch, use the command `git checkout -b <branch-name>`. Replace `<branch-name>` with the name you want to give to your new branch. This command will create a new branch and switch you to it.\\n\\n4. **Push the new branch to the remote repository**: If you want to push your new branch to the remote repository (e.g., GitHub, GitLab, Bitbucket), use the command `git push -u origin <branch-name>`. This will push your new branch to the remote repository and set the upstream tracking.\\n\\nRemember to replace `<branch-name>` with your desired branch name in the commands above.\\n\\nAs for resources, here are a few that might help:\\n\\n1. **dbt Documentation**: The official dbt documentation is a great place to start. It provides detailed information on how to use dbt and its features.\\n\\n2. **Git Documentation**: The official Git documentation is a comprehensive resource for learning about Git and its commands.\\n\\n3. **Interactive Git Tutorials**: Websites like [Learn Git Branching](https://learngitbranching.js.org/) offer interactive tutorials that can help you understand and practice Git commands, including creating and managing branches.\\n\\n4. **dbt Community Slack**: The dbt Community Slack is a place where you can ask questions and learn from other dbt users. You can find the invitation link on the dbt website.\\n\\n5. **Online Courses and Tutorials**: Websites like Udemy, Coursera, and LinkedIn Learning offer courses on dbt and Git that can help you learn these tools in a structured way.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 325,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 451,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:17+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': '4dd82f4c90c346408fbc3b2b59dc29a9',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203017_a0cdb4.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': \"How can I set the `Project subdirectory` in dbt cloud? I can't seem to find the option.\",\n",
       "      'timestamp': '2025-10-03T20:30:33.121155+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'tool_name': 'text_search',\n",
       "      'args': '{\"query\": \"set the `Project subdirectory` in dbt cloud\"}',\n",
       "      'tool_call_id': 'Fsbjg4JPs',\n",
       "      'part_kind': 'tool-call'}],\n",
       "    'usage': {'input_tokens': 324,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 21,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:33+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'tool_calls'},\n",
       "    'provider_response_id': '2640d7a20df2403ab0ebdf2c904f0308',\n",
       "    'finish_reason': 'tool_call'},\n",
       "   {'parts': [{'tool_name': 'text_search',\n",
       "      'content': [{'id': 'c7ca760fed',\n",
       "        'images': [{'description': 'image #1',\n",
       "          'id': 'image_1',\n",
       "          'path': 'images/data-engineering-zoomcamp/image_25bb53c3.png'}],\n",
       "        'question': 'How to set subdirectory of the github repository as the dbt project root',\n",
       "        'sort_order': 42,\n",
       "        'content': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:\\n\\n<{IMAGE:image_1}>',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/042_c7ca760fed_how-to-set-subdirectory-of-the-github-repository-a.md'},\n",
       "       {'id': 'fee87c684a',\n",
       "        'question': 'DBT Deploy: Error When trying to run the dbt project on Prod',\n",
       "        'sort_order': 69,\n",
       "        'content': 'When running the dbt project on production, ensure the following steps:\\n\\n1. **Pull Request and Merge**\\n   - Make the pull request and merge the branch into the main.\\n\\n2. **Version Check**\\n   - Ensure you have the latest version if changes were made to the repository elsewhere.\\n\\n3. **Project File Accessibility**\\n   - Verify that the `dbt_project.yml` file is accessible to the project. If not, refer to the solution for the error: \"Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.\"\\n\\n4. **Dataset Consistency**\\n   - Confirm that the name assigned to the dataset on BigQuery matches the name specified in the production environment configuration on dbt Cloud.',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/069_fee87c684a_dbt-deploy-error-when-trying-to-run-the-dbt-projec.md'},\n",
       "       {'id': 'dc51ef9830',\n",
       "        'images': [{'description': 'image #1',\n",
       "          'id': 'image_1',\n",
       "          'path': 'images/data-engineering-zoomcamp/image_c27ecb8e.png'},\n",
       "         {'description': 'image #2',\n",
       "          'id': 'image_2',\n",
       "          'path': 'images/data-engineering-zoomcamp/image_8b6478c1.png'},\n",
       "         {'description': 'image #3',\n",
       "          'id': 'image_3',\n",
       "          'path': 'images/data-engineering-zoomcamp/image_80992235.png'},\n",
       "         {'description': 'image #4',\n",
       "          'id': 'image_4',\n",
       "          'path': 'images/data-engineering-zoomcamp/image_cd924928.png'}],\n",
       "        'question': 'DBT Deploy: This dbt Cloud run was cancelled because a valid dbt project was not found.',\n",
       "        'sort_order': 67,\n",
       "        'content': 'This issue occurs when the dbt project is moved to another directory in the repository or if you\\'re on a different branch than expected.\\n\\n**Solution:**\\n\\n1. Navigate to the projects window on dbt Cloud.\\n2. Go to Settings -> Edit.\\n3. Add the directory path where the dbt project is located. Ensure that this path matches your file explorer path. For example:\\n   \\n   ```\\n   /week5/taxi_rides_ny\\n   ```\\n\\n4. Check that there are no files waiting to be committed to GitHub if you’re running the job to deploy to PROD.\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>\\n\\n5. Ensure the PROD environment is set up to check the main branch, or the specified branch.\\n\\nIn the image below, the branch \"ella2024\" is set to be checked as \"production-ready\" by the \"freshness\" check mark in PROD environment settings. Each time a branch is merged into \"ella2024\" and a PR is triggered, the CI check job initiates. Note that merging and closing the PR must be done manually.\\n\\n<{IMAGE:image_3}>\\n\\n6. Set up the PROD custom branch (if not the default main) in the Environment setup screen.\\n\\n<{IMAGE:image_4}>',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/067_dc51ef9830_dbt-deploy-this-dbt-cloud-run-was-cancelled-becaus.md'},\n",
       "       {'id': '44ff6ae3df',\n",
       "        'question': 'dbt cloud Developer',\n",
       "        'sort_order': 2,\n",
       "        'content': \"Please be aware that the demos are done using dbt cloud Developer licensing. Although Team license is available to you upon creation of dbt cloud account for 14 days, the interface won't fully match the demo-ed experience.\",\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/002_44ff6ae3df_dbt-cloud-developer.md'},\n",
       "       {'id': '5c5321c295',\n",
       "        'question': 'Project 1 & Project 2',\n",
       "        'sort_order': 4,\n",
       "        'content': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects.\\n\\nThere are simply TWO chances to pass the course. You can use the Second Attempt if you:\\n\\n- Fail the first attempt\\n- Do not have the time due to other engagements such as holidays or sickness to enter your project into the first attempt.\\n\\n**Project Evaluation - Reproducibility**\\n\\nEven with thorough documentation, ensuring that a peer reviewer can follow your steps can be challenging. Here’s how this criterion will be evaluated:\\n\\n> \"Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions, and so on - then it\\'s already great.\"\\n\\n**Certificates: How do I get it?**\\n\\nSee the `certificate.mdx` file.',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/004_5c5321c295_project-1-project-2.md'}],\n",
       "      'tool_call_id': 'Fsbjg4JPs',\n",
       "      'metadata': None,\n",
       "      'timestamp': '2025-10-03T20:30:33.543349+00:00',\n",
       "      'part_kind': 'tool-return'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"To set the `Project subdirectory` in dbt Cloud, follow these steps:\\n\\n1. Navigate to the projects window on dbt Cloud.\\n2. Go to Settings -> Edit.\\n3. Add the directory path where the dbt project is located. Ensure that this path matches your file explorer path. For example:\\n\\n   ```\\n   /week5/taxi_rides_ny\\n   ```\\n\\n4. Check that there are no files waiting to be committed to GitHub if you’re running the job to deploy to PROD.\\n\\nFor more detailed instructions, refer to the [DBT Deploy: This dbt Cloud run was cancelled because a valid dbt project was not found](faq-main/_questions/data-engineering-zoomcamp/module-4/067_dc51ef9830_dbt-deploy-this-dbt-cloud-run-was-cancelled-becaus.md) guide.\\n\\n---\\n\\n### Checklist Evaluation\\n\\n- **instructions_follow**: true - The response provides steps to set the `Project subdirectory` in dbt Cloud.\\n- **instructions_avoid**: true - There are no instructions to avoid in this context.\\n- **answer_relevant**: true - The response directly addresses the user's question about setting the `Project subdirectory`.\\n- **answer_clear**: true - The answer is clear and provides a step-by-step guide.\\n- **answer_citations**: true - The response includes a reference to a detailed guide.\\n- **completeness**: true - The response covers all key aspects of setting the `Project subdirectory`.\\n- **tool_call_search**: true - The search tool was invoked to find relevant information.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 1671,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 340,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:33+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': '1d8a5647ef874653b72e37b6e1300ef2',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203033_6a8b6e.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': \"I'm having trouble building the `fact_trips.sql` model. The issue seems to be with the `payment_type_description` field. How can I resolve this?\",\n",
       "      'timestamp': '2025-10-03T20:30:23.604397+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"I'm sorry to hear that you're having trouble with building the `fact_trips.sql` model, specifically with the `payment_type_description` field. Unfortunately, I don't have access to the necessary tools to provide a specific solution for your issue. However, I can suggest some general steps you might take to troubleshoot the problem:\\n\\n1. **Check for Typos**: Ensure that there are no typos in your SQL code, especially around the `payment_type_description` field.\\n2. **Verify Data Types**: Make sure that the data type of `payment_type_description` is compatible with the operations you're performing.\\n3. **Review Dependencies**: Check if there are any dependencies or constraints associated with the `payment_type_description` field that might be causing the issue.\\n4. **Consult Documentation**: Look up the documentation or any available resources related to the software or database you're using.\\n5. **Seek Community Help**: Consider reaching out to community forums or support groups where you might find others who have encountered similar issues.\\n\\nIf you provide more details about the error message or the specific problem you're encountering, I might be able to offer more targeted advice.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 335,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 239,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:23+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': '43cf0abda8eb464caca65bed73d324fc',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203023_8f2534.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': 'The audio quality in the course videos is hard to hear. Do you have any suggestions for improving it?',\n",
       "      'timestamp': '2025-10-03T20:30:20.114534+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"I'm sorry to hear that you're having trouble with the audio quality in the course videos. Unfortunately, I don't have the tools to directly address this issue. However, I can suggest a few general tips that might help improve your listening experience:\\n\\n1. **Check Your Internet Connection**: Ensure that your internet connection is stable, as this can affect the streaming quality of the videos.\\n2. **Use Headphones**: Sometimes, using headphones can provide a clearer audio experience compared to speakers.\\n3. **Adjust Video Quality**: If the platform allows, try lowering the video quality to see if this improves the audio.\\n4. **Contact Support**: Reach out to the course provider's support team for assistance. They may have specific solutions or alternatives for you.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 321,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 154,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:20+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': '791c4b07d3c54959a3184a0d769d3b33',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203020_521b2d.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': 'What are the prerequisites for this data engineering course? Do I need prior experience in data engineering?',\n",
       "      'timestamp': '2025-10-03T20:30:32.428082+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"I'm sorry, but I don't have access to the specific information needed to answer your question about the prerequisites for the data engineering course. However, I can suggest that you check the course website or contact the course provider directly for the most accurate and detailed information. They should be able to provide you with insights into whether prior experience in data engineering is required.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 320,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 74,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:32+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': '070d06229dad4d83903580fcdf6d4a4e',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203032_d6544a.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': \"I'm encountering a port conflict when trying to run a service on port 8080. How can I free up this port?\",\n",
       "      'timestamp': '2025-10-03T20:30:26.152246+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"I'm sorry to hear you're experiencing a port conflict. Here are a few steps you can take to free up port 8080:\\n\\n1. **Identify the Process Using the Port**: You can use the `netstat` command to identify which process is using the port. On Windows, you can use `netstat -ano | findstr :8080` and on Unix-based systems, you can use `netstat -tuln | grep 8080`.\\n\\n2. **Terminate the Process**: Once you've identified the process, you can terminate it. On Windows, you can use the `Taskkill` command with the process ID (PID) that you found in the previous step. On Unix-based systems, you can use the `kill` command with the PID.\\n\\n3. **Change the Port**: If you can't terminate the process or if it's a system process, you might want to consider changing the port that your service is trying to use. You can usually do this in the configuration files of your service.\\n\\n4. **Use a Different Port**: If changing the port isn't an option, you might want to consider using a different port for your service. You can choose a port that's not currently in use.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 329,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 262,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:26+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': 'e0e8520fd0314315b98ad71dbac60294',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203026_7b91c4.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': 'Can I set up the course environment on my local machine? Are there any specific challenges for Windows users? What are the alternatives if I prefer not to work locally?',\n",
       "      'timestamp': '2025-10-03T20:30:42.876998+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"To evaluate the quality of the AI agent's answer, let's go through the checklist:\\n\\n1. **instructions_follow**: The agent followed the user's instructions to provide information on setting up the course environment locally, challenges for Windows users, and alternatives to working locally.\\n   - **Judgment**: true\\n   - **Explanation**: The agent addressed all parts of the user's question.\\n\\n2. **instructions_avoid**: The agent avoided doing things it was told not to do.\\n   - **Judgment**: true\\n   - **Explanation**: There were no specific instructions to avoid, but the agent did not provide any irrelevant information.\\n\\n3. **answer_relevant**: The response directly addresses the user's question about setting up the course environment locally, challenges for Windows users, and alternatives to working locally.\\n   - **Judgment**: true\\n   - **Explanation**: The agent provided relevant information on all aspects of the user's question.\\n\\n4. **answer_clear**: The answer is clear and correct.\\n   - **Judgment**: true\\n   - **Explanation**: The agent's response was clear and provided correct information.\\n\\n5. **answer_citations**: The response includes proper citations or sources when required.\\n   - **Judgment**: false\\n   - **Explanation**: The agent did not provide any citations or sources for the information given.\\n\\n6. **completeness**: The response is complete and covers all key aspects of the request.\\n   - **Judgment**: true\\n   - **Explanation**: The agent covered all parts of the user's question comprehensively.\\n\\n7. **tool_call_search**: Is the search tool invoked?\\n   - **Judgment**: false\\n   - **Explanation**: The agent did not invoke the search tool to gather more information.\\n\\nOverall, the AI agent's answer was comprehensive, relevant, and clear, but it lacked citations or sources for the information provided.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 333,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 392,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:43+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': 'e417eff8490946fdbf86d31091b37abd',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203043_5d2e25.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': \"I'm having trouble accessing Dataproc in my Google Cloud project. What steps can I take to resolve this issue?\",\n",
       "      'timestamp': '2025-10-03T20:30:38.437501+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': \"I'm sorry to hear that you're having trouble accessing Dataproc. Unfortunately, I don't have the necessary tools or information to provide you with specific steps to resolve this issue. However, I can suggest that you check the Google Cloud status page for any ongoing outages or issues, ensure that your project has the necessary permissions and APIs enabled, and consult the Google Cloud documentation or support for further assistance.\",\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 324,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 83,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:38+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': 'b94cfeb5b14f41abbdd81c654a2067b2',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203038_a5c4a7.json')},\n",
       " {'agent_name': 'faq_agent_v3',\n",
       "  'system_prompt': [\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\"],\n",
       "  'provider': 'mistral',\n",
       "  'model': 'mistral-small-latest',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': \"I'm trying to create a new server in pgAdmin but the create server dialog is not appearing. What should I do?\",\n",
       "      'timestamp': '2025-10-03T20:30:39.370614+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'tool_name': 'text_search',\n",
       "      'args': '{\"query\": \"create server dialog not appearing in pgAdmin\"}',\n",
       "      'tool_call_id': 'dHzfEtwrk',\n",
       "      'part_kind': 'tool-call'}],\n",
       "    'usage': {'input_tokens': 325,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 18,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:40+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'tool_calls'},\n",
       "    'provider_response_id': '197ab9c7671c402a850dfa41b1404178',\n",
       "    'finish_reason': 'tool_call'},\n",
       "   {'parts': [{'tool_name': 'text_search',\n",
       "      'content': [{'id': '117164c439',\n",
       "        'question': 'pgAdmin: Create server dialog does not appear',\n",
       "        'sort_order': 78,\n",
       "        'content': 'pgAdmin has a new version. The create server dialog may not appear. Try using `Register` -> `Server` instead.',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/078_117164c439_pgadmin-create-server-dialog-does-not-appear.md'},\n",
       "       {'id': 'ca3b8ac8db',\n",
       "        'question': 'pgAdmin: How to Persist pgAdmin Configurations',\n",
       "        'sort_order': 81,\n",
       "        'content': \"To keep pgAdmin settings after restarting the container, follow these steps:\\n\\n1. Create the directory for pgAdmin data:\\n   \\n   ```bash\\n   mkdir -p /path/to/pgadmin-data\\n   ```\\n\\n2. Assign ownership to pgAdmin's user (ID 5050):\\n   \\n   ```bash\\n   sudo chown -R 5050:5050 /path/to/pgadmin-data\\n   ```\\n\\n3. Set the appropriate permissions:\\n   \\n   ```bash\\n   sudo chmod -R 755 /path/to/pgadmin-data\\n   ```\",\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/081_ca3b8ac8db_pgadmin-how-to-persist-pgadmin-configurations.md'},\n",
       "       {'id': '2efd03d7f8',\n",
       "        'question': 'pgAdmin - Can not access/open the PgAdmin address via browser',\n",
       "        'sort_order': 80,\n",
       "        'content': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when trying to run the PgAdmin container via `docker run` or `docker compose`, I couldn\\'t access the PgAdmin address via my browser. After modifications, I was able to access it.\\n\\n### Solution #1:\\n\\nModify the `docker run` command:\\n\\n```bash\\ndocker run --rm -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n  -e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n  -e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n  -e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n  -e PGADMIN_LISTEN_PORT=5050 \\\\\\n  -p 5050:5050 \\\\\\n  --network=de-zoomcamp-network \\\\\\n  --name pgadmin-container \\\\\\n  --link postgres-container \\\\\\n  -t dpage/pgadmin4\\n```\\n\\n### Solution #2:\\n\\nModify the `docker-compose.yaml` configuration and use the `docker compose up` command:\\n\\n```yaml\\npgadmin:\\n  image: dpage/pgadmin4\\n  container_name: pgadmin-container\\n  environment:\\n    - PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n    - PGADMIN_DEFAULT_PASSWORD=pgadmin\\n    - PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n    - PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n    - PGADMIN_LISTEN_PORT=5050\\n  volumes:\\n    - \"./pgadmin_data:/var/lib/pgadmin/data\"\\n  ports:\\n    - \"5050:5050\"\\n  networks:\\n    - de-zoomcamp-network\\n  depends_on:\\n    - postgres-container\\n```',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/080_2efd03d7f8_pgadmin-can-not-accessopen-the-pgadmin-address-via.md'},\n",
       "       {'id': '3d24f7796d',\n",
       "        'question': 'GitHub Codespaces: Running pgadmin in Docker',\n",
       "        'sort_order': 9,\n",
       "        'content': 'With the default instructions, running pgadmin in Docker may result in a blank screen after logging into the pgadmin console. To resolve this, add the following two environment variables to your pgadmin configuration to allow it to work with Codespaces’ reverse proxy:\\n\\n```plaintext\\nPGADMIN_CONFIG_PROXY_X_HOST_COUNT: 1\\nPGADMIN_CONFIG_PROXY_X_PREFIX_COUNT: 1\\n```',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/009_3d24f7796d_github-codespaces-running-pgadmin-in-docker.md'},\n",
       "       {'id': 'ac52bea382',\n",
       "        'question': 'Docker-Compose: Persist PGAdmin docker contents on GCP',\n",
       "        'sort_order': 42,\n",
       "        'content': 'One common issue when running Docker Compose on GCP is that PostgreSQL might not persist its data to the specified path. For example:\\n\\n```yaml\\nservices:\\n  ...\\n  pgadmin:\\n    ...\\n    volumes:\\n      - \"./pgadmin:/var/lib/pgadmin:wr\"\\n```\\n\\nThis setup might not work. To resolve this, use Docker Volume to make the data persist:\\n\\n```yaml\\nservices:\\n  ...\\n  pgadmin:\\n    ...\\n    volumes:\\n      - pgadmin:/var/lib/pgadmin\\n\\nvolumes:\\n  pgadmin:\\n```\\n\\nThis configuration change ensures the persistence of the PGAdmin data on GCP.',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/042_ac52bea382_docker-compose-persist-pgadmin-docker-contents-on.md'}],\n",
       "      'tool_call_id': 'dHzfEtwrk',\n",
       "      'metadata': None,\n",
       "      'timestamp': '2025-10-03T20:30:41.061628+00:00',\n",
       "      'part_kind': 'tool-return'}],\n",
       "    'instructions': \"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nFor each item, check if the condition is met.\\n\\nChecklist:\\n\\n- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\\n\\nOutput true/false for each check and provide a short explanation for your judgment.\",\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': '### pgAdmin: Create Server Dialog Does Not Appear\\n\\npgAdmin has a new version. The create server dialog may not appear. Try using `Register` -> `Server` instead.\\n\\n### Additional Resources\\n\\n- [How to Persist pgAdmin Configurations](faq-main/_questions/data-engineering-zoomcamp/module-1/081_ca3b8ac8db_pgadmin-how-to-persist-pgadmin-configurations.md)\\n- [Accessing pgAdmin Address via Browser](faq-main/_questions/data-engineering-zoomcamp/module-1/080_2efd03d7f8_pgadmin-can-not-accessopen-the-pgadmin-address-via.md)\\n- [Running pgAdmin in Docker with GitHub Codespaces](faq-main/_questions/data-engineering-zoomcamp/module-1/009_3d24f7796d_github-codespaces-running-pgadmin-in-docker.md)\\n- [Persisting PGAdmin Docker Contents on GCP](faq-main/_questions/data-engineering-zoomcamp/module-1/042_ac52bea382_docker-compose-persist-pgadmin-docker-contents-on.md)',\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 1602,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 254,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {}},\n",
       "    'model_name': 'mistral-small-latest',\n",
       "    'timestamp': '2025-10-03T20:30:41+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'mistral',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': 'f043dd9b85444386b0ee333bb37dad73',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': PosixPath('logs/faq_agent_v3_20251003_203041_0d819a.json')}]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "61f368bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce233bb331b49d5b838dcd70047eb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "21de45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06600d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a5c57bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    1.0\n",
       "instructions_avoid     0.9\n",
       "answer_relevant        1.0\n",
       "answer_clear           1.0\n",
       "answer_citations       0.5\n",
       "completeness           0.8\n",
       "tool_call_search       0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152236cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Day_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
