{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d31b2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Day 4: Agents and Tools\n",
    "\n",
    "Today we are going to explore the ways we can enable agents to use tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e03b8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's add the markdown download code here\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md')\n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd2fed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from course!\n",
      "FAQ documents: 3\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello from course!\")\n",
    "balsam_faq = read_repo_data('Zesky665', 'balsam')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "    \n",
    "print(f\"FAQ documents: {len(balsam_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e888fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can start chunking\n",
    "# There are multiple ways of chunking a document.\n",
    "# Here they are in order of complexity:\n",
    "# - Simple Chunking\n",
    "# - Token Based Chunking\n",
    "# - Sematinc Chunking\n",
    "# - Paragrapgh Splitting\n",
    "# - Section Splitting\n",
    "# - AI-powered Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d59cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most commonly used simple chunking method is sliding window, which is chunking with overlap.\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be4304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy['content']\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd29cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section Splitting\n",
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecc25c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "\n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "\n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8232e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e8567c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MISTRAL_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmistralai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mistral\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m api_key=\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMISTRAL_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m model = \u001b[33m\"\u001b[39m\u001b[33mmistral-large-latest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m client = Mistral(api_key=api_key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:685\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'MISTRAL_API_KEY'"
     ]
    }
   ],
   "source": [
    "# LLM Based Chunking\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key=os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def llm(prompt, model=\"mistral-large-latest\"):\n",
    "    chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the best French cheese?\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    print(chat_response.choices[0].message.content)\n",
    "    return chat_response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4208e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c3da327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt_template)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f28d04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cc6ea329054a55aca0758df924159d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m doc_copy = doc.copy()\n\u001b[32m      7\u001b[39m doc_content = doc_copy.pop(\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m sections = \u001b[43mintelligent_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m section \u001b[38;5;129;01min\u001b[39;00m sections:\n\u001b[32m     11\u001b[39m     section_doc = doc_copy.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mintelligent_chunking\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintelligent_chunking\u001b[39m(text):\n\u001b[32m      2\u001b[39m     prompt = prompt_template.format(document=text)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mllm\u001b[49m(prompt_template)\n\u001b[32m      4\u001b[39m     sections = response.split(\u001b[33m'\u001b[39m\u001b[33m---\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     sections = [s.strip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sections \u001b[38;5;28;01mif\u001b[39;00m s.strip()]\n",
      "\u001b[31mNameError\u001b[39m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search\n",
    "# Text, Vector and Sematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e65662c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7943eaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x115ffec90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Search\n",
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0c402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7afd10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68d1e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acc16eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d61aad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a548e15bb24cf3a2f2958d6d2c8ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf28f2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x150cb93a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5561eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8de9668c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48d8e5d999e49f385f61f13253cda87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x1305b7e30>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0017d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0156ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "\n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2ae44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents and Tool Use\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key= \"tzrdB1gE59KBnu3zt0o3dSHhGzsOXfCi\"#os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-small-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def llm(prompt, model=\"mistral-small-latest\"):\n",
    "    chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    print(chat_response.choices[0].message.content)\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f838ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! You can join the course now. Here’s what you can do next:\n",
      "\n",
      "1. **Enroll**: If the course is on a platform like Coursera, Udemy, or edX, simply click the \"Enroll\" or \"Join Now\" button.\n",
      "2. **Access Materials**: Once enrolled, you’ll get immediate access to the course content (videos, readings, assignments, etc.).\n",
      "3. **Start Learning**: Begin with the first module or lesson—most courses are self-paced, so you can go at your own speed.\n",
      "\n",
      "If you have any questions about the course structure, prerequisites, or payment, feel free to ask! Welcome aboard—happy learning! 🎓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Of course! You can join the course now. Here’s what you can do next:\\n\\n1. **Enroll**: If the course is on a platform like Coursera, Udemy, or edX, simply click the \"Enroll\" or \"Join Now\" button.\\n2. **Access Materials**: Once enrolled, you’ll get immediate access to the course content (videos, readings, assignments, etc.).\\n3. **Start Learning**: Begin with the first module or lesson—most courses are self-paced, so you can go at your own speed.\\n\\nIf you have any questions about the course structure, prerequisites, or payment, feel free to ask! Welcome aboard—happy learning! 🎓'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "resp = llm(user_prompt, model)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d83872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfe665e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"text_search\",\n",
    "        \"description\": \"Search the FAQ database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7c5cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "tools = [text_search_tool]\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = chat_messages,\n",
    "        tools = tools,\n",
    "        tool_choice = \"any\",\n",
    "        parallel_tool_calls = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce1d7aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='b4c0d3ddc3d5433680f7431ad606c6c8', object='chat.completion', model='mistral-small-latest', usage=UsageInfo(prompt_tokens=104, completion_tokens=16, total_tokens=120, prompt_audio_seconds=Unset()), created=1759444129, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='text_search', arguments='{\"query\": \"can I join the course now\"}'), id='zl9Jjin3B', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd059b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "call = chat_response.choices[0].message.tool_calls[0]\n",
    "\n",
    "arguments = json.loads(call.function.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.id,\n",
    "    \"output\": json.dumps(result),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9b0bdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function_call_output',\n",
       " 'call_id': 'zl9Jjin3B',\n",
       " 'output': '[{\"id\": \"3f1424af17\", \"question\": \"Course: Can I still join the course after the start date?\", \"sort_order\": 3, \"content\": \"Yes, even if you don\\'t register, you\\'re still eligible to submit the homework.\\\\n\\\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don\\'t leave everything for the last minute.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\"}, {\"id\": \"9e508f2212\", \"question\": \"Course: When does the course start?\", \"sort_order\": 1, \"content\": \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\\\n\\\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\\\n- Don\\\\u2019t forget to register in DataTalks.Club\\'s Slack and join the channel.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\"}, {\"id\": \"068529125b\", \"question\": \"Course - Can I follow the course after it finishes?\", \"sort_order\": 8, \"content\": \"Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\\\n\\\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\"}, {\"id\": \"33fc260cd8\", \"question\": \"Course: What can I do before the course starts?\", \"sort_order\": 5, \"content\": \"Start by installing and setting up all the dependencies and requirements:\\\\n\\\\n- Google Cloud account\\\\n- Google Cloud SDK\\\\n- Python 3 (installed with Anaconda)\\\\n- Terraform\\\\n- Git\\\\n\\\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md\"}, {\"id\": \"c207b8614e\", \"question\": \"Course: Can I get support if I take the course in the self-paced mode?\", \"sort_order\": 9, \"content\": \"Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\\\n\\\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don\\\\u2019t rely on its answers 100%.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\"}]'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2cfa8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='c48ace4bd12a412b8b6fe8ef53c21976' object='chat.completion' model='mistral-small-latest' usage=UsageInfo(prompt_tokens=2624, completion_tokens=50, total_tokens=2674, prompt_audio_seconds=Unset()) created=1759444228 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n"
     ]
    }
   ],
   "source": [
    "# Append the assistant's message with tool calls\n",
    "chat_messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": [call]\n",
    "})\n",
    "\n",
    "# Append the tool result\n",
    "chat_messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"name\": call.function.name,\n",
    "    \"content\": call_output[\"output\"],\n",
    "    \"tool_call_id\": call.id\n",
    "})\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = chat_messages,\n",
    "        tools = tools,\n",
    "        tool_choice = \"auto\",\n",
    "        parallel_tool_calls = False,\n",
    "    )\n",
    "\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d51a26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = chat_messages,\n",
    "        tools = tools,\n",
    "        tool_choice = \"auto\",\n",
    "        parallel_tool_calls = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eb4239d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='296c142f8ac440459fa59eb4f6cae741', object='chat.completion', model='mistral-small-latest', usage=UsageInfo(prompt_tokens=104, completion_tokens=16, total_tokens=120, prompt_audio_seconds=Unset()), created=1759445822, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='text_search', arguments='{\"query\": \"can I join the course now\"}'), id='v2oNBJdjZ', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f16244f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "58384906",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Always search for relevant information before answering.\n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da0cbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ccc2c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "import os\n",
    "os.environ['MISTRAL_API_KEY'] = api_key\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='mistral:mistral-small-latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5dc1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2dca8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"Yes, you can still join the course after the start date. You can submit homework even if you don't register, but be aware of the deadlines for homework and final projects. Don't leave everything for the last minute.\")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eab74b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2025, 10, 2, 23, 5, 25, 669076, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course.\\n\\nAlways search for relevant information before answering.\\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\": \"can I join the course now\"}', tool_call_id='1Yo0fJswk')], usage=RequestUsage(input_tokens=179, output_tokens=16), model_name='mistral-small-latest', timestamp=datetime.datetime(2025, 10, 2, 23, 5, 25, tzinfo=TzInfo(UTC)), provider_name='mistral', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='103cc7f4ee7543eaad28fbb1395481f8', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}], tool_call_id='1Yo0fJswk', timestamp=datetime.datetime(2025, 10, 2, 23, 5, 26, 105256, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course.\\n\\nAlways search for relevant information before answering.\\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[TextPart(content=\"Yes, you can still join the course after the start date. You can submit homework even if you don't register, but be aware of the deadlines for homework and final projects. Don't leave everything for the last minute.\")], usage=RequestUsage(input_tokens=973, output_tokens=47), model_name='mistral-small-latest', timestamp=datetime.datetime(2025, 10, 2, 23, 5, 26, tzinfo=TzInfo(UTC)), provider_name='mistral', provider_details={'finish_reason': 'stop'}, provider_response_id='f3263383de004b37814aa2719e03b9a2', finish_reason='stop')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Day_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
